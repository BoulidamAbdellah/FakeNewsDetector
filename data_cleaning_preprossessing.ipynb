{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nettoyage et Prétraitement des Données pour la Détection de Fausses Nouvelles\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Ce notebook détaille les étapes cruciales de nettoyage et de prétraitement des données textuelles issues des ensembles de nouvelles \"Fake.csv\" et \"True.csv\". L'objectif principal est de transformer le texte brut en un format structuré et normalisé, propice à l'extraction de caractéristiques (features) pertinentes pour l'entraînement de modèles d'apprentissage automatique capables de détecter les fausses nouvelles.\n",
    "\n",
    "Le processus comprend plusieurs phases :\n",
    "1.  **Chargement et exploration initiale des données** : Pour comprendre la structure et le contenu des jeux de données.\n",
    "2.  **Gestion des mots vides (stopwords)** : Création d'une liste exhaustive de mots vides à partir de multiples sources pour les éliminer du texte.\n",
    "3.  **Traitement par lots (chunks)** : Application d'un pipeline de nettoyage (passage en minuscules, suppression de la ponctuation et des caractères non pertinents, lemmatisation) de manière efficace en termes de mémoire.\n",
    "4.  **Nettoyage final des DataFrames** : Gestion des valeurs manquantes et suppression des colonnes inutiles.\n",
    "5.  **Extraction et sauvegarde des caractéristiques TF-IDF** : Vectorisation des textes en utilisant TF-IDF et calcul de la similarité cosinus entre titres et textes.\n",
    "6.  **Extraction et sauvegarde des caractéristiques Doc2Vec** : Apprentissage de plongements de documents (Doc2Vec) et calcul de la similarité cosinus.\n",
    "7.  **Combinaison et sauvegarde des ensembles de caractéristiques finaux** : Préparation des matrices de caractéristiques pour la phase de modélisation.\n",
    "\n",
    "Chaque étape vise à améliorer la qualité des données d'entrée pour les modèles, en réduisant le bruit et en standardisant le format du texte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning (racinisation, suppression des stop words et ponctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialisation et Chargement des Données\n",
    "\n",
    "### Importation des Bibliothèques\n",
    "Cette section commence par l'importation des bibliothèques Python nécessaires pour la manipulation des données, le traitement du langage naturel (NLP) et l'apprentissage automatique.\n",
    "gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "\n",
    "### Configuration Initiale de spaCy et NLTK\n",
    "Le modèle `en_core_web_sm` de spaCy est chargé pour les opérations NLP, et les mots vides (stopwords) anglais de NLTK sont téléchargés et affichés à titre d'exemple.\n",
    "\n",
    "\n",
    "### Chargement des Jeux de Données Bruts\n",
    "Les jeux de données `Fake.csv` et `True.csv` sont chargés dans des DataFrames Pandas. Une première inspection est effectuée à l'aide de la méthode `.head()` pour visualiser les premières lignes et comprendre la structure des données (colonnes: `title`, `text`, `subject`, `date`).\n",
    "*Note : Dans le code initial, `Fake.csv` est chargé pour `document_True_df` également. Pour la suite du traitement, il est supposé que `True.csv` est bien utilisé pour les \"vraies nouvelles\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from joblib import dump\n",
    "import string\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "import re \n",
    "import spacy\n",
    "import joblib \n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nltk.download('stopwords')\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "subject",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "date",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "4eb1d486-2724-43bf-9f42-e9c115aa2562",
       "rows": [
        [
         "0",
         " Donald Trump Sends Out Embarrassing New Year’s Eve Message; This is Disturbing",
         "Donald Trump just couldn t wish all Americans a Happy New Year and leave it at that. Instead, he had to give a shout out to his enemies, haters and  the very dishonest fake news media.  The former reality show star had just one job to do and he couldn t do it. As our Country rapidly grows stronger and smarter, I want to wish all of my friends, supporters, enemies, haters, and even the very dishonest Fake News Media, a Happy and Healthy New Year,  President Angry Pants tweeted.  2018 will be a great year for America! As our Country rapidly grows stronger and smarter, I want to wish all of my friends, supporters, enemies, haters, and even the very dishonest Fake News Media, a Happy and Healthy New Year. 2018 will be a great year for America!  Donald J. Trump (@realDonaldTrump) December 31, 2017Trump s tweet went down about as welll as you d expect.What kind of president sends a New Year s greeting like this despicable, petty, infantile gibberish? Only Trump! His lack of decency won t even allow him to rise above the gutter long enough to wish the American citizens a happy new year!  Bishop Talbert Swan (@TalbertSwan) December 31, 2017no one likes you  Calvin (@calvinstowell) December 31, 2017Your impeachment would make 2018 a great year for America, but I ll also accept regaining control of Congress.  Miranda Yaver (@mirandayaver) December 31, 2017Do you hear yourself talk? When you have to include that many people that hate you you have to wonder? Why do the they all hate me?  Alan Sandoval (@AlanSandoval13) December 31, 2017Who uses the word Haters in a New Years wish??  Marlene (@marlene399) December 31, 2017You can t just say happy new year?  Koren pollitt (@Korencarpenter) December 31, 2017Here s Trump s New Year s Eve tweet from 2016.Happy New Year to all, including to my many enemies and those who have fought me and lost so badly they just don t know what to do. Love!  Donald J. Trump (@realDonaldTrump) December 31, 2016This is nothing new for Trump. He s been doing this for years.Trump has directed messages to his  enemies  and  haters  for New Year s, Easter, Thanksgiving, and the anniversary of 9/11. pic.twitter.com/4FPAe2KypA  Daniel Dale (@ddale8) December 31, 2017Trump s holiday tweets are clearly not presidential.How long did he work at Hallmark before becoming President?  Steven Goodine (@SGoodine) December 31, 2017He s always been like this . . . the only difference is that in the last few years, his filter has been breaking down.  Roy Schulze (@thbthttt) December 31, 2017Who, apart from a teenager uses the term haters?  Wendy (@WendyWhistles) December 31, 2017he s a fucking 5 year old  Who Knows (@rainyday80) December 31, 2017So, to all the people who voted for this a hole thinking he would change once he got into power, you were wrong! 70-year-old men don t change and now he s a year older.Photo by Andrew Burton/Getty Images.",
         "News",
         "December 31, 2017"
        ],
        [
         "1",
         " Drunk Bragging Trump Staffer Started Russian Collusion Investigation",
         "House Intelligence Committee Chairman Devin Nunes is going to have a bad day. He s been under the assumption, like many of us, that the Christopher Steele-dossier was what prompted the Russia investigation so he s been lashing out at the Department of Justice and the FBI in order to protect Trump. As it happens, the dossier is not what started the investigation, according to documents obtained by the New York Times.Former Trump campaign adviser George Papadopoulos was drunk in a wine bar when he revealed knowledge of Russian opposition research on Hillary Clinton.On top of that, Papadopoulos wasn t just a covfefe boy for Trump, as his administration has alleged. He had a much larger role, but none so damning as being a drunken fool in a wine bar. Coffee boys  don t help to arrange a New York meeting between Trump and President Abdel Fattah el-Sisi of Egypt two months before the election. It was known before that the former aide set up meetings with world leaders for Trump, but team Trump ran with him being merely a coffee boy.In May 2016, Papadopoulos revealed to Australian diplomat Alexander Downer that Russian officials were shopping around possible dirt on then-Democratic presidential nominee Hillary Clinton. Exactly how much Mr. Papadopoulos said that night at the Kensington Wine Rooms with the Australian, Alexander Downer, is unclear,  the report states.  But two months later, when leaked Democratic emails began appearing online, Australian officials passed the information about Mr. Papadopoulos to their American counterparts, according to four current and former American and foreign officials with direct knowledge of the Australians  role. Papadopoulos pleaded guilty to lying to the F.B.I. and is now a cooperating witness with Special Counsel Robert Mueller s team.This isn t a presidency. It s a badly scripted reality TV show.Photo by Win McNamee/Getty Images.",
         "News",
         "December 31, 2017"
        ],
        [
         "2",
         " Sheriff David Clarke Becomes An Internet Joke For Threatening To Poke People ‘In The Eye’",
         "On Friday, it was revealed that former Milwaukee Sheriff David Clarke, who was being considered for Homeland Security Secretary in Donald Trump s administration, has an email scandal of his own.In January, there was a brief run-in on a plane between Clarke and fellow passenger Dan Black, who he later had detained by the police for no reason whatsoever, except that maybe his feelings were hurt. Clarke messaged the police to stop Black after he deplaned, and now, a search warrant has been executed by the FBI to see the exchanges.Clarke is calling it fake news even though copies of the search warrant are on the Internet. I am UNINTIMIDATED by lib media attempts to smear and discredit me with their FAKE NEWS reports designed to silence me,  the former sheriff tweeted.  I will continue to poke them in the eye with a sharp stick and bitch slap these scum bags til they get it. I have been attacked by better people than them #MAGA I am UNINTIMIDATED by lib media attempts to smear and discredit me with their FAKE NEWS reports designed to silence me. I will continue to poke them in the eye with a sharp stick and bitch slap these scum bags til they get it. I have been attacked by better people than them #MAGA pic.twitter.com/XtZW5PdU2b  David A. Clarke, Jr. (@SheriffClarke) December 30, 2017He didn t stop there.BREAKING NEWS! When LYING LIB MEDIA makes up FAKE NEWS to smear me, the ANTIDOTE is go right at them. Punch them in the nose & MAKE THEM TASTE THEIR OWN BLOOD. Nothing gets a bully like LYING LIB MEDIA S attention better than to give them a taste of their own blood #neverbackdown pic.twitter.com/T2NY2psHCR  David A. Clarke, Jr. (@SheriffClarke) December 30, 2017The internet called him out.This is your local newspaper and that search warrant isn t fake, and just because the chose not to file charges at the time doesn t mean they won t! Especially if you continue to lie. Months after decision not to charge Clarke, email search warrant filed https://t.co/zcbyc4Wp5b  KeithLeBlanc (@KeithLeBlanc63) December 30, 2017I just hope the rest of the Village People aren t implicated.  Kirk Ketchum (@kirkketchum) December 30, 2017Slaw, baked potatoes, or French fries? pic.twitter.com/fWfXsZupxy  ALT- Immigration   (@ALT_uscis) December 30, 2017pic.twitter.com/ymsOBLjfxU  Pendulum Swinger (@PendulumSwngr) December 30, 2017you called your police friends to stand up for you when someone made fun of your hat  Chris Jackson (@ChrisCJackson) December 30, 2017Is it me, with this masterful pshop of your hat, which I seem to never tire of. I think it s the steely resolve in your one visible eye pic.twitter.com/dWr5k8ZEZV  Chris Mohney (@chrismohney) December 30, 2017Are you indicating with your fingers how many people died in your jail? I think you re a few fingers short, dipshit  Ike Barinholtz (@ikebarinholtz) December 30, 2017ROFL. Internet tough guy with fake flair. pic.twitter.com/ulCFddhkdy  KellMeCrazy (@Kel_MoonFace) December 30, 2017You re so edgy, buddy.  Mrs. SMH (@MRSSMH2) December 30, 2017Is his break over at Applebees?  Aaron (@feltrrr2) December 30, 2017Are you trying to earn your  still relevant  badge?  CircusRebel (@CircusDrew) December 30, 2017make sure to hydrate, drink lots of water. It s rumored that prisoners can be denied water by prison officials.  Robert Klinc (@RobertKlinc1) December 30, 2017Terrill Thomas, the 38-year-old black man who died of thirst in Clarke s Milwaukee County Jail cell this April, was a victim of homicide. We just thought we should point that out. It can t be repeated enough.Photo by Spencer Platt/Getty Images.",
         "News",
         "December 30, 2017"
        ],
        [
         "3",
         " Trump Is So Obsessed He Even Has Obama’s Name Coded Into His Website (IMAGES)",
         "On Christmas day, Donald Trump announced that he would  be back to work  the following day, but he is golfing for the fourth day in a row. The former reality show star blasted former President Barack Obama for playing golf and now Trump is on track to outpace the number of golf games his predecessor played.Updated my tracker of Trump s appearances at Trump properties.71 rounds of golf including today s. At this pace, he ll pass Obama s first-term total by July 24 next year. https://t.co/Fg7VacxRtJ pic.twitter.com/5gEMcjQTbH  Philip Bump (@pbump) December 29, 2017 That makes what a Washington Post reporter discovered on Trump s website really weird, but everything about this administration is bizarre AF. The coding contained a reference to Obama and golf:  Unlike Obama, we are working to fix the problem   and not on the golf course.  However, the coding wasn t done correctly.The website of Donald Trump, who has spent several days in a row at the golf course, is coded to serve up the following message in the event of an internal server error: https://t.co/zrWpyMXRcz pic.twitter.com/wiQSQNNzw0  Christopher Ingraham (@_cingraham) December 28, 2017That snippet of code appears to be on all https://t.co/dkhw0AlHB4 pages, which the footer says is paid for by the RNC? pic.twitter.com/oaZDT126B3  Christopher Ingraham (@_cingraham) December 28, 2017It s also all over https://t.co/ayBlGmk65Z. As others have noted in this thread, this is weird code and it s not clear it would ever actually display, but who knows.  Christopher Ingraham (@_cingraham) December 28, 2017After the coding was called out, the reference to Obama was deleted.UPDATE: The golf error message has been removed from the Trump and GOP websites. They also fixed the javascript  =  vs  ==  problem. Still not clear when these messages would actually display, since the actual 404 (and presumably 500) page displays a different message pic.twitter.com/Z7dmyQ5smy  Christopher Ingraham (@_cingraham) December 29, 2017That suggests someone at either RNC or the Trump admin is sensitive enough to Trump s golf problem to make this issue go away quickly once people noticed. You have no idea how much I d love to see the email exchange that led us here.  Christopher Ingraham (@_cingraham) December 29, 2017 The code was f-cked up.The best part about this is that they are using the  =  (assignment) operator which means that bit of code will never get run. If you look a few lines up  errorCode  will always be  404          (@tw1trsux) December 28, 2017trump s coders can t code. Nobody is surprised.  Tim Peterson (@timrpeterson) December 28, 2017Donald Trump is obsessed with Obama that his name was even in the coding of his website while he played golf again.Photo by Joe Raedle/Getty Images.",
         "News",
         "December 29, 2017"
        ],
        [
         "4",
         " Pope Francis Just Called Out Donald Trump During His Christmas Speech",
         "Pope Francis used his annual Christmas Day message to rebuke Donald Trump without even mentioning his name. The Pope delivered his message just days after members of the United Nations condemned Trump s move to recognize Jerusalem as the capital of Israel. The Pontiff prayed on Monday for the  peaceful coexistence of two states within mutually agreed and internationally recognized borders. We see Jesus in the children of the Middle East who continue to suffer because of growing tensions between Israelis and Palestinians,  Francis said.  On this festive day, let us ask the Lord for peace for Jerusalem and for all the Holy Land. Let us pray that the will to resume dialogue may prevail between the parties and that a negotiated solution can finally be reached. The Pope went on to plead for acceptance of refugees who have been forced from their homes, and that is an issue Trump continues to fight against. Francis used Jesus for which there was  no place in the inn  as an analogy. Today, as the winds of war are blowing in our world and an outdated model of development continues to produce human, societal and environmental decline, Christmas invites us to focus on the sign of the Child and to recognize him in the faces of little children, especially those for whom, like Jesus,  there is no place in the inn,  he said. Jesus knows well the pain of not being welcomed and how hard it is not to have a place to lay one s head,  he added.  May our hearts not be closed as they were in the homes of Bethlehem. The Pope said that Mary and Joseph were immigrants who struggled to find a safe place to stay in Bethlehem. They had to leave their people, their home, and their land,  Francis said.  This was no comfortable or easy journey for a young couple about to have a child.   At heart, they were full of hope and expectation because of the child about to be born; yet their steps were weighed down by the uncertainties and dangers that attend those who have to leave their home behind. So many other footsteps are hidden in the footsteps of Joseph and Mary,  Francis said Sunday. We see the tracks of entire families forced to set out in our own day. We see the tracks of millions of persons who do not choose to go away, but driven from their land, leave behind their dear ones. Amen to that.Photo by Christopher Furlong/Getty Images.",
         "News",
         "December 25, 2017"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date  \n",
       "0  December 31, 2017  \n",
       "1  December 31, 2017  \n",
       "2  December 30, 2017  \n",
       "3  December 29, 2017  \n",
       "4  December 25, 2017  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_Fake_df = pd.read_csv(\"Fake.csv\")\n",
    "document_True_df = pd.read_csv(\"Fake.csv\") \n",
    "(document_Fake_df.head())\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "subject",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "date",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "300d8b33-3525-4473-a8e4-386ffe30481b",
       "rows": [
        [
         "0",
         " Donald Trump Sends Out Embarrassing New Year’s Eve Message; This is Disturbing",
         "Donald Trump just couldn t wish all Americans a Happy New Year and leave it at that. Instead, he had to give a shout out to his enemies, haters and  the very dishonest fake news media.  The former reality show star had just one job to do and he couldn t do it. As our Country rapidly grows stronger and smarter, I want to wish all of my friends, supporters, enemies, haters, and even the very dishonest Fake News Media, a Happy and Healthy New Year,  President Angry Pants tweeted.  2018 will be a great year for America! As our Country rapidly grows stronger and smarter, I want to wish all of my friends, supporters, enemies, haters, and even the very dishonest Fake News Media, a Happy and Healthy New Year. 2018 will be a great year for America!  Donald J. Trump (@realDonaldTrump) December 31, 2017Trump s tweet went down about as welll as you d expect.What kind of president sends a New Year s greeting like this despicable, petty, infantile gibberish? Only Trump! His lack of decency won t even allow him to rise above the gutter long enough to wish the American citizens a happy new year!  Bishop Talbert Swan (@TalbertSwan) December 31, 2017no one likes you  Calvin (@calvinstowell) December 31, 2017Your impeachment would make 2018 a great year for America, but I ll also accept regaining control of Congress.  Miranda Yaver (@mirandayaver) December 31, 2017Do you hear yourself talk? When you have to include that many people that hate you you have to wonder? Why do the they all hate me?  Alan Sandoval (@AlanSandoval13) December 31, 2017Who uses the word Haters in a New Years wish??  Marlene (@marlene399) December 31, 2017You can t just say happy new year?  Koren pollitt (@Korencarpenter) December 31, 2017Here s Trump s New Year s Eve tweet from 2016.Happy New Year to all, including to my many enemies and those who have fought me and lost so badly they just don t know what to do. Love!  Donald J. Trump (@realDonaldTrump) December 31, 2016This is nothing new for Trump. He s been doing this for years.Trump has directed messages to his  enemies  and  haters  for New Year s, Easter, Thanksgiving, and the anniversary of 9/11. pic.twitter.com/4FPAe2KypA  Daniel Dale (@ddale8) December 31, 2017Trump s holiday tweets are clearly not presidential.How long did he work at Hallmark before becoming President?  Steven Goodine (@SGoodine) December 31, 2017He s always been like this . . . the only difference is that in the last few years, his filter has been breaking down.  Roy Schulze (@thbthttt) December 31, 2017Who, apart from a teenager uses the term haters?  Wendy (@WendyWhistles) December 31, 2017he s a fucking 5 year old  Who Knows (@rainyday80) December 31, 2017So, to all the people who voted for this a hole thinking he would change once he got into power, you were wrong! 70-year-old men don t change and now he s a year older.Photo by Andrew Burton/Getty Images.",
         "News",
         "December 31, 2017"
        ],
        [
         "1",
         " Drunk Bragging Trump Staffer Started Russian Collusion Investigation",
         "House Intelligence Committee Chairman Devin Nunes is going to have a bad day. He s been under the assumption, like many of us, that the Christopher Steele-dossier was what prompted the Russia investigation so he s been lashing out at the Department of Justice and the FBI in order to protect Trump. As it happens, the dossier is not what started the investigation, according to documents obtained by the New York Times.Former Trump campaign adviser George Papadopoulos was drunk in a wine bar when he revealed knowledge of Russian opposition research on Hillary Clinton.On top of that, Papadopoulos wasn t just a covfefe boy for Trump, as his administration has alleged. He had a much larger role, but none so damning as being a drunken fool in a wine bar. Coffee boys  don t help to arrange a New York meeting between Trump and President Abdel Fattah el-Sisi of Egypt two months before the election. It was known before that the former aide set up meetings with world leaders for Trump, but team Trump ran with him being merely a coffee boy.In May 2016, Papadopoulos revealed to Australian diplomat Alexander Downer that Russian officials were shopping around possible dirt on then-Democratic presidential nominee Hillary Clinton. Exactly how much Mr. Papadopoulos said that night at the Kensington Wine Rooms with the Australian, Alexander Downer, is unclear,  the report states.  But two months later, when leaked Democratic emails began appearing online, Australian officials passed the information about Mr. Papadopoulos to their American counterparts, according to four current and former American and foreign officials with direct knowledge of the Australians  role. Papadopoulos pleaded guilty to lying to the F.B.I. and is now a cooperating witness with Special Counsel Robert Mueller s team.This isn t a presidency. It s a badly scripted reality TV show.Photo by Win McNamee/Getty Images.",
         "News",
         "December 31, 2017"
        ],
        [
         "2",
         " Sheriff David Clarke Becomes An Internet Joke For Threatening To Poke People ‘In The Eye’",
         "On Friday, it was revealed that former Milwaukee Sheriff David Clarke, who was being considered for Homeland Security Secretary in Donald Trump s administration, has an email scandal of his own.In January, there was a brief run-in on a plane between Clarke and fellow passenger Dan Black, who he later had detained by the police for no reason whatsoever, except that maybe his feelings were hurt. Clarke messaged the police to stop Black after he deplaned, and now, a search warrant has been executed by the FBI to see the exchanges.Clarke is calling it fake news even though copies of the search warrant are on the Internet. I am UNINTIMIDATED by lib media attempts to smear and discredit me with their FAKE NEWS reports designed to silence me,  the former sheriff tweeted.  I will continue to poke them in the eye with a sharp stick and bitch slap these scum bags til they get it. I have been attacked by better people than them #MAGA I am UNINTIMIDATED by lib media attempts to smear and discredit me with their FAKE NEWS reports designed to silence me. I will continue to poke them in the eye with a sharp stick and bitch slap these scum bags til they get it. I have been attacked by better people than them #MAGA pic.twitter.com/XtZW5PdU2b  David A. Clarke, Jr. (@SheriffClarke) December 30, 2017He didn t stop there.BREAKING NEWS! When LYING LIB MEDIA makes up FAKE NEWS to smear me, the ANTIDOTE is go right at them. Punch them in the nose & MAKE THEM TASTE THEIR OWN BLOOD. Nothing gets a bully like LYING LIB MEDIA S attention better than to give them a taste of their own blood #neverbackdown pic.twitter.com/T2NY2psHCR  David A. Clarke, Jr. (@SheriffClarke) December 30, 2017The internet called him out.This is your local newspaper and that search warrant isn t fake, and just because the chose not to file charges at the time doesn t mean they won t! Especially if you continue to lie. Months after decision not to charge Clarke, email search warrant filed https://t.co/zcbyc4Wp5b  KeithLeBlanc (@KeithLeBlanc63) December 30, 2017I just hope the rest of the Village People aren t implicated.  Kirk Ketchum (@kirkketchum) December 30, 2017Slaw, baked potatoes, or French fries? pic.twitter.com/fWfXsZupxy  ALT- Immigration   (@ALT_uscis) December 30, 2017pic.twitter.com/ymsOBLjfxU  Pendulum Swinger (@PendulumSwngr) December 30, 2017you called your police friends to stand up for you when someone made fun of your hat  Chris Jackson (@ChrisCJackson) December 30, 2017Is it me, with this masterful pshop of your hat, which I seem to never tire of. I think it s the steely resolve in your one visible eye pic.twitter.com/dWr5k8ZEZV  Chris Mohney (@chrismohney) December 30, 2017Are you indicating with your fingers how many people died in your jail? I think you re a few fingers short, dipshit  Ike Barinholtz (@ikebarinholtz) December 30, 2017ROFL. Internet tough guy with fake flair. pic.twitter.com/ulCFddhkdy  KellMeCrazy (@Kel_MoonFace) December 30, 2017You re so edgy, buddy.  Mrs. SMH (@MRSSMH2) December 30, 2017Is his break over at Applebees?  Aaron (@feltrrr2) December 30, 2017Are you trying to earn your  still relevant  badge?  CircusRebel (@CircusDrew) December 30, 2017make sure to hydrate, drink lots of water. It s rumored that prisoners can be denied water by prison officials.  Robert Klinc (@RobertKlinc1) December 30, 2017Terrill Thomas, the 38-year-old black man who died of thirst in Clarke s Milwaukee County Jail cell this April, was a victim of homicide. We just thought we should point that out. It can t be repeated enough.Photo by Spencer Platt/Getty Images.",
         "News",
         "December 30, 2017"
        ],
        [
         "3",
         " Trump Is So Obsessed He Even Has Obama’s Name Coded Into His Website (IMAGES)",
         "On Christmas day, Donald Trump announced that he would  be back to work  the following day, but he is golfing for the fourth day in a row. The former reality show star blasted former President Barack Obama for playing golf and now Trump is on track to outpace the number of golf games his predecessor played.Updated my tracker of Trump s appearances at Trump properties.71 rounds of golf including today s. At this pace, he ll pass Obama s first-term total by July 24 next year. https://t.co/Fg7VacxRtJ pic.twitter.com/5gEMcjQTbH  Philip Bump (@pbump) December 29, 2017 That makes what a Washington Post reporter discovered on Trump s website really weird, but everything about this administration is bizarre AF. The coding contained a reference to Obama and golf:  Unlike Obama, we are working to fix the problem   and not on the golf course.  However, the coding wasn t done correctly.The website of Donald Trump, who has spent several days in a row at the golf course, is coded to serve up the following message in the event of an internal server error: https://t.co/zrWpyMXRcz pic.twitter.com/wiQSQNNzw0  Christopher Ingraham (@_cingraham) December 28, 2017That snippet of code appears to be on all https://t.co/dkhw0AlHB4 pages, which the footer says is paid for by the RNC? pic.twitter.com/oaZDT126B3  Christopher Ingraham (@_cingraham) December 28, 2017It s also all over https://t.co/ayBlGmk65Z. As others have noted in this thread, this is weird code and it s not clear it would ever actually display, but who knows.  Christopher Ingraham (@_cingraham) December 28, 2017After the coding was called out, the reference to Obama was deleted.UPDATE: The golf error message has been removed from the Trump and GOP websites. They also fixed the javascript  =  vs  ==  problem. Still not clear when these messages would actually display, since the actual 404 (and presumably 500) page displays a different message pic.twitter.com/Z7dmyQ5smy  Christopher Ingraham (@_cingraham) December 29, 2017That suggests someone at either RNC or the Trump admin is sensitive enough to Trump s golf problem to make this issue go away quickly once people noticed. You have no idea how much I d love to see the email exchange that led us here.  Christopher Ingraham (@_cingraham) December 29, 2017 The code was f-cked up.The best part about this is that they are using the  =  (assignment) operator which means that bit of code will never get run. If you look a few lines up  errorCode  will always be  404          (@tw1trsux) December 28, 2017trump s coders can t code. Nobody is surprised.  Tim Peterson (@timrpeterson) December 28, 2017Donald Trump is obsessed with Obama that his name was even in the coding of his website while he played golf again.Photo by Joe Raedle/Getty Images.",
         "News",
         "December 29, 2017"
        ],
        [
         "4",
         " Pope Francis Just Called Out Donald Trump During His Christmas Speech",
         "Pope Francis used his annual Christmas Day message to rebuke Donald Trump without even mentioning his name. The Pope delivered his message just days after members of the United Nations condemned Trump s move to recognize Jerusalem as the capital of Israel. The Pontiff prayed on Monday for the  peaceful coexistence of two states within mutually agreed and internationally recognized borders. We see Jesus in the children of the Middle East who continue to suffer because of growing tensions between Israelis and Palestinians,  Francis said.  On this festive day, let us ask the Lord for peace for Jerusalem and for all the Holy Land. Let us pray that the will to resume dialogue may prevail between the parties and that a negotiated solution can finally be reached. The Pope went on to plead for acceptance of refugees who have been forced from their homes, and that is an issue Trump continues to fight against. Francis used Jesus for which there was  no place in the inn  as an analogy. Today, as the winds of war are blowing in our world and an outdated model of development continues to produce human, societal and environmental decline, Christmas invites us to focus on the sign of the Child and to recognize him in the faces of little children, especially those for whom, like Jesus,  there is no place in the inn,  he said. Jesus knows well the pain of not being welcomed and how hard it is not to have a place to lay one s head,  he added.  May our hearts not be closed as they were in the homes of Bethlehem. The Pope said that Mary and Joseph were immigrants who struggled to find a safe place to stay in Bethlehem. They had to leave their people, their home, and their land,  Francis said.  This was no comfortable or easy journey for a young couple about to have a child.   At heart, they were full of hope and expectation because of the child about to be born; yet their steps were weighed down by the uncertainties and dangers that attend those who have to leave their home behind. So many other footsteps are hidden in the footsteps of Joseph and Mary,  Francis said Sunday. We see the tracks of entire families forced to set out in our own day. We see the tracks of millions of persons who do not choose to go away, but driven from their land, leave behind their dear ones. Amen to that.Photo by Christopher Furlong/Getty Images.",
         "News",
         "December 25, 2017"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date  \n",
       "0  December 31, 2017  \n",
       "1  December 31, 2017  \n",
       "2  December 30, 2017  \n",
       "3  December 29, 2017  \n",
       "4  December 25, 2017  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(document_True_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combinaison et préparation des stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Combinaison et Préparation des Mots Vides (Stopwords)\n",
    "\n",
    "Afin d'éliminer efficacement les mots courants qui n'apportent que peu de valeur sémantique pour la classification, une liste complète de mots vides est construite en agrégeant ceux de plusieurs bibliothèques populaires : NLTK, spaCy, scikit-learn et Gensim.\n",
    "\n",
    "### Fonctions Utilitaires\n",
    "- `clean_word(word)`: Une fonction pour nettoyer un mot en ne conservant que les caractères alphabétiques et en le convertissant en minuscules.\n",
    "- `get_combined_stopwords()`: Combine les listes de mots vides des différentes sources, nettoie chaque mot vide à l'aide de `clean_word`, et retourne une liste triée unique de mots vides.\n",
    "\n",
    "Une instance de spaCy est chargée spécifiquement pour cette étape, en désactivant les composants non nécessaires (`parser`, `ner`, `lemmatizer`) pour optimiser le chargement.\n",
    "t(s_words)\n",
    "\n",
    "La variable globale `s_words` contient désormais la liste exhaustive de mots vides qui sera utilisée dans les étapes de nettoyage ultérieures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stopwords\n",
    "from gensim.parsing.preprocessing import STOPWORDS as gensim_stopwords\n",
    "\n",
    "# 1. Initialisation rapide de spaCy (sans composants inutiles)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\", \"lemmatizer\"])\n",
    "\n",
    "def clean_word(word):\n",
    "    \"\"\"Nettoie le mot : conserve uniquement les lettres alphabétiques et met en minuscule\"\"\"\n",
    "    return re.sub(r'[^a-z]', '', word.lower())\n",
    "\n",
    "def get_combined_stopwords():\n",
    "    \"\"\"Combine les stopwords de toutes les sources et les nettoie\"\"\"\n",
    "    # Sources de stopwords\n",
    "    sources = {\n",
    "        \"nltk\": set(nltk_stopwords.words('english')),\n",
    "        \"spacy\": set(nlp.Defaults.stop_words),\n",
    "        \"sklearn\": set(sklearn_stopwords),\n",
    "        \"gensim\": set(gensim_stopwords)\n",
    "    }\n",
    "    \n",
    "    # Combinaison et nettoyage\n",
    "    combined = set()\n",
    "    for source_words in sources.values():\n",
    "        for word in source_words:\n",
    "            cleaned = clean_word(word)  # Nettoie le mot\n",
    "            if cleaned and len(cleaned):  # Ignore les mots vides ou trop courts\n",
    "                combined.add(cleaned)\n",
    "    \n",
    "    return sorted(combined)\n",
    "s_words = get_combined_stopwords()\n",
    "print((s_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traitement des dataframes par chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Traitement des DataFrames par Lots (Chunks)\n",
    "\n",
    "Pour gérer efficacement la mémoire lors du traitement de potentiellement grands volumes de texte, un pipeline de nettoyage est appliqué aux DataFrames par lots (chunks).\n",
    "\n",
    "### Initialisation de spaCy pour la Lemmatisation\n",
    "Une instance du modèle `en_core_web_sm` de spaCy est chargée. Pour la lemmatisation (`token.lemma_`), il est important que le composant \"lemmatizer\" soit actif. L'initialisation `nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])` active le lemmatiseur par défaut tout en désactivant les autres composants non requis pour cette tâche, optimisant ainsi les performances.\n",
    "\n",
    "### Fonction `process_dataframe_in_chunks`\n",
    "Cette fonction prend un chemin de fichier CSV, le nom de la colonne textuelle à traiter, la taille des lots et un chemin de sortie. Pour chaque lot :\n",
    "1.  Le texte est converti en minuscules.\n",
    "2.  Les caractères non alphabétiques (sauf les espaces) sont supprimés via une expression régulière.\n",
    "3.  Les documents du lot sont traités par `nlp.pipe()` de spaCy pour une tokenisation et une lemmatisation efficaces.\n",
    "4.  Pour chaque document traité, les tokens sont filtrés : les mots vides (contenus dans `s_words`) et la ponctuation sont supprimés. Les tokens de longueur 1 sont également ignorés. Les lemmes des tokens restants sont joints pour former le texte nettoyé.\n",
    "5.  Les résultats du lot sont écrits dans le fichier de sortie (en mode écriture pour le premier lot, puis en mode ajout pour les suivants).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "\n",
    "# 1. Initialisation rapide de spaCy (sans composants inutiles)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\", \"lemmatizer\"])\n",
    "\n",
    "# Chargement du modèle une seule fois\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "# Traitement par petits lots pour préserver la mémoire\n",
    "def process_dataframe_in_chunks(file_path, text_column, chunk_size=1000, output_path=\"dataset_processed_all_fake.csv\"):\n",
    "    global s_words\n",
    "    # Initialiser le fichier de sortie\n",
    "    first_chunk = True\n",
    "    \n",
    "    # Traiter le fichier par morceaux\n",
    "    for chunk in tqdm(pd.read_csv(file_path, chunksize=chunk_size)):\n",
    "        # Nettoyage basique (vectorisé)\n",
    "        chunk[text_column] = chunk[text_column].str.lower()\n",
    "        chunk[text_column] = chunk[text_column].str.replace(r'[^a-z\\s]', '', regex=True)\n",
    "        \n",
    "        # Traitement spaCy (sans parallélisation excessive)\n",
    "        docs = list(nlp.pipe(chunk[text_column], batch_size=100))\n",
    "        \n",
    "        # Extraire les tokens filtrés\n",
    "        chunk[f\"processed_{text_column}\"] = [\n",
    "            \" \".join(token.lemma_ for token in doc \n",
    "                    if not token.text in s_words and not token.is_punct and len(token.text) > 1)\n",
    "            for doc in docs\n",
    "        ]\n",
    "        \n",
    "        # Écrire les résultats sans tout garder en mémoire\n",
    "        mode = 'w' if first_chunk else 'a'\n",
    "        header = first_chunk\n",
    "        chunk.to_csv(output_path, mode=mode, header=header, index=False)\n",
    "        first_chunk = False\n",
    "        \n",
    "        # Libérer la mémoire explicitement\n",
    "        del docs\n",
    "        \n",
    "# Exécution\n",
    "process_dataframe_in_chunks(\"Fake.csv\", text_column=\"text\", chunk_size=500,output_path = \"dataset_processed.csv\")\n",
    "process_dataframe_in_chunks(\"dataset_processed.csv\", text_column=\"title\", chunk_size=500,output_path = \"dataset_processed_all_fake.csv\")\n",
    "df = pd.read_csv(\"dataset_processed_all_fake.csv\")\n",
    "del df[\"title\"]\n",
    "del df[\"text\"]\n",
    "df.to_csv(\"dataset_Fake_final.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dataframe_in_chunks(\"True.csv\", text_column=\"text\", chunk_size=500,output_path = \"dataset_processed_true.csv\")\n",
    "process_dataframe_in_chunks(\"dataset_processed_true.csv\", text_column=\"title\", chunk_size=500,output_path = \"dataset_processed_all_true.csv\")\n",
    "df = pd.read_csv(\"dataset_processed_all_true.csv\")\n",
    "del df[\"title\"]\n",
    "del df[\"text\"]\n",
    "df.to_csv(\"dataste_True_final.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization TF-IDF + similarité cosinus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Vectorisation TF-IDF et Calcul de Similarité\n",
    "\n",
    "### Initialisation et Entraînement du Vectoriseur TF-IDF\n",
    "La technique TF-IDF (Term Frequency-Inverse Document Frequency) est utilisée pour convertir les textes prétraités en une représentation numérique vectorielle.\n",
    "- Un `TfidfVectorizer` de la bibliothèque scikit-learn est initialisé. Les paramètres clés sont :\n",
    "    - `ngram_range=(1,2)` : pour considérer à la fois les mots individuels (unigrammes) et les paires de mots consécutifs (bigrammes) comme des caractéristiques.\n",
    "    - `min_df=50` : pour ignorer les termes qui apparaissent dans moins de 50 documents, aidant à filtrer le bruit et à réduire la dimensionnalité.\n",
    "- Un DataFrame `df_entrainement` est créé en concaténant les colonnes `processed_text` et `processed_title` des deux jeux de données (`dataset_fake` et `dataset_true`). Cette collection de tous les textes disponibles sert à construire le vocabulaire du vectoriseur TF-IDF.\n",
    "- Le vectoriseur est ensuite \"entraîné\" (ajusté) sur ces données combinées avec la méthode `.fit()`.\n",
    "- Pour une réutilisation future sans ré-entraînement, le vectoriseur ajusté est sauvegardé sur disque à l'aide de `joblib.dump()` et immédiatement rechargé avec `joblib.load()`. La taille du vocabulaire appris (nombre de caractéristiques uniques) est affichée.\n",
    "\n",
    "```python\n",
    "tf_idf_vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=50)\n",
    "\n",
    "# Création du DataFrame d'entraînement pour le vocabulaire TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_fake = pd.read_csv(\"dataset_Fake_final.csv\")\n",
    "dataset_true = pd.read_csv(\"dataset_True_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0           0\n",
       "subject              0\n",
       "date                 0\n",
       "processed_text     627\n",
       "processed_title      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_fake.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset_fake[\"Unnamed: 0\"]\n",
    "del dataset_true[\"Unnamed: 0\"]\n",
    "del dataset_true[\"Unnamed: 0.1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_fake.dropna(inplace=True)\n",
    "dataset_true.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22854, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_fake.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vectorizer = TfidfVectorizer(ngram_range=(1,2),min_df=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entrainement = pd.DataFrame()\n",
    "df_entrainement = pd.concat([df_entrainement,dataset_fake[\"processed_text\"],dataset_fake[\"processed_title\"],\n",
    "           dataset_true[\"processed_text\"],dataset_true[\"processed_title\"]],ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vectorizer.fit(df_entrainement[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(tf_idf_vectorizer,\"tf_idf_vectorizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vectorizer = joblib.load(\"tf_idf_vectorizer.pkl\")\n",
    "len(tf_idf_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_fake_vect = tf_idf_vectorizer.transform(dataset_fake[\"processed_title\"])\n",
    "text_fake_vect = tf_idf_vectorizer.transform(dataset_fake[\"processed_text\"])\n",
    "title_true_vect = tf_idf_vectorizer.transform(dataset_true[\"processed_title\"])\n",
    "text_true_vect = tf_idf_vectorizer.transform(dataset_true[\"processed_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Exemple sur les fake news :\n",
    "similarity_fake = cosine_similarity(title_fake_vect, text_fake_vect).diagonal()\n",
    "# Exemple sur les true news :\n",
    "similarity_true = cosine_similarity(title_true_vect, text_true_vect).diagonal()\n",
    "# Maintenant, tu veux ajouter cette similarité en tant que nouvelle \"colonne\" (feature)\n",
    "# On transforme similarity en matrice sparse compatible\n",
    "similarity_fake_vect = sp.csr_matrix(similarity_fake).T  # transpose pour en faire une colonne\n",
    "similarity_true_vect = sp.csr_matrix(similarity_true).T\n",
    "# Puis on concatène horizontalement\n",
    "fake_features = sp.hstack([title_fake_vect, text_fake_vect, similarity_fake_vect])\n",
    "true_features = sp.hstack([title_true_vect, text_true_vect, similarity_true_vect])\n",
    "\n",
    "print(fake_features.shape)  # (nombre d'articles, taille_title + taille_text + 1)\n",
    "print(true_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import save_npz\n",
    "\n",
    "save_npz('fake_features.npz', fake_features)\n",
    "save_npz('true_features.npz', true_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22854, 43504)\n",
      "(21416, 43504)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import load_npz\n",
    "\n",
    "fake_features = load_npz('fake_features.npz')\n",
    "true_features = load_npz('true_features.npz')\n",
    "\n",
    "print(fake_features.shape)  # (nombre d'articles, taille_title + taille_text + 1)\n",
    "print(true_features.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec feature extraction + similarité cosinus (Doc2Vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Extraction de Caractéristiques Doc2Vec et Calcul de Similarité\n",
    "\n",
    "En complément de TF-IDF, la méthode Doc2Vec est utilisée pour apprendre des représentations vectorielles (plongements) de documents qui capturent mieux la sémantique.\n",
    "\n",
    "### Entraînement du Modèle Doc2Vec\n",
    "- Les documents utilisés pour entraîner le vectoriseur TF-IDF (`df_entrainement_content` qui contient tous les titres et textes prétraités) sont d'abord formatés en objets `TaggedDocument`. Chaque document est associé à un tag unique (ici, un identifiant numérique sous forme de chaîne).\n",
    "- Un modèle `Doc2Vec` de la bibliothèque Gensim est initialisé avec des paramètres tels que `vector_size=100` (dimension des vecteurs), `window=2` (taille de la fenêtre de contexte), `min_count=1` (fréquence minimale des mots), `workers=4` (threads pour l'entraînement) et `epochs=100` (nombre de passes sur les données).\n",
    "- Le vocabulaire du modèle Doc2Vec est construit à partir des `tagged_data`.\n",
    "- Le modèle est ensuite entraîné sur ces mêmes `tagged_data`.\n",
    "- Une fois l'entraînement terminé, le modèle Doc2Vec est sauvegardé sur disque et immédiatement rechargé pour une utilisation ultérieure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tagging each document (tag can be any unique identifier, here we're using integers)\n",
    "tagged_data = [TaggedDocument(words=doc.split(), tags=[str(i)]) for i, doc in enumerate(df_entrainement[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Initialize the model\n",
    "model = Doc2Vec(vector_size=100, window=2, min_count=1, workers=4, epochs=100)\n",
    "\n",
    "# Build the vocabulary\n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "# Train the model\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le modèle\n",
    "model.save(\"mon_modele_doc2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec.load(\"mon_modele_doc2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création et sauvegarde des features finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_decvect(text):\n",
    " vector = model.infer_vector(text.split())\n",
    "# Afficher le vecteur      \n",
    " return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "def infer_vectors_chunked(dataset, column_name, chunk_size=1000):\n",
    "    \"\"\"\n",
    "    Traite un DataFrame déjà en mémoire par morceaux pour éviter les problèmes de mémoire\n",
    "    \n",
    "    Args:\n",
    "        dataset: DataFrame pandas contenant les données\n",
    "        column_name: Nom de la colonne à traiter\n",
    "        chunk_size: Taille des morceaux à traiter\n",
    "    \n",
    "    Returns:\n",
    "        Array numpy avec les vecteurs inférés\n",
    "    \"\"\"\n",
    "    total_rows = len(dataset)\n",
    "    all_vectors = []\n",
    "    \n",
    "    # Calculer le nombre de morceaux\n",
    "    num_chunks = (total_rows + chunk_size - 1) // chunk_size\n",
    "    \n",
    "    # Traiter par morceaux\n",
    "    for i in tqdm(range(0, total_rows, chunk_size), desc=f\"Traitement de {column_name}\", total=num_chunks):\n",
    "        # Extraire un morceau\n",
    "        end_idx = min(i + chunk_size, total_rows)\n",
    "        chunk = dataset.iloc[i:end_idx]\n",
    "        \n",
    "        # Appliquer la fonction d'inférence vectorielle\n",
    "        chunk_vectors = np.array([text_to_decvect(doc) for doc in chunk[column_name]])\n",
    "        \n",
    "        # Stocker les résultats\n",
    "        all_vectors.append(chunk_vectors)\n",
    "        \n",
    "        # Afficher la progression\n",
    "        print(f\"Morceau {i//chunk_size + 1}/{num_chunks} traité, forme: {chunk_vectors.shape}\")\n",
    "        \n",
    "        # Libérer la mémoire\n",
    "        del chunk_vectors\n",
    "        gc.collect()\n",
    "    \n",
    "    # Concaténer tous les vecteurs\n",
    "    if all_vectors:\n",
    "        result = np.vstack(all_vectors)\n",
    "        print(f\"Traitement terminé pour {column_name}. Forme finale: {result.shape}\")\n",
    "        return result\n",
    "    else:\n",
    "        return np.array([])\n",
    "\n",
    "# Utiliser la fonction sur vos données\n",
    "title_fake_docvect = infer_vectors_chunked(dataset_fake, \"processed_title\", chunk_size=500)\n",
    "text_fake_docvect = infer_vectors_chunked(dataset_fake, \"processed_text\", chunk_size=500)\n",
    "title_true_docvect = infer_vectors_chunked(dataset_true, \"processed_title\", chunk_size=500)\n",
    "text_true_docvect = infer_vectors_chunked(dataset_true, \"processed_text\", chunk_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inférence des Vecteurs Doc2Vec et Calcul de la Similarité Cosinus\n",
    "- Une fonction `text_to_d2v_vector(text, model_instance)` est définie (ou réutilisée/adaptée de `text_to_decvect`) pour inférer le vecteur Doc2Vec d'un texte donné. Elle prend le texte et l'instance du modèle Doc2Vec entraîné en arguments et utilise `model_instance.infer_vector()`.\n",
    "- La fonction `infer_vectors_chunked` (définie précédemment dans le notebook) est utilisée pour générer les vecteurs Doc2Vec pour les colonnes `processed_title` et `processed_text` des DataFrames `dataset_fake` et `dataset_true`. Cette fonction doit être adaptée pour passer l'instance `model_d2v` à la fonction d'inférence vectorielle interne.\n",
    "- Après avoir obtenu les vecteurs Doc2Vec pour les titres et les textes, la similarité cosinus est calculée entre chaque paire titre-texte.\n",
    "- Ces scores de similarité sont transformés en matrices creuses d'une seule colonne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Exemple sur les fake news :\n",
    "similarity_fake = cosine_similarity(title_fake_docvect, text_fake_docvect).diagonal()\n",
    "# Exemple sur les true news :\n",
    "similarity_true = cosine_similarity(title_true_docvect, text_true_docvect).diagonal()\n",
    "# Maintenant, tu veux ajouter cette similarité en tant que nouvelle \"colonne\" (feature)\n",
    "# On transforme similarity en matrice sparse compatible\n",
    "similarity_fake_doc_vect = sp.csr_matrix(similarity_fake).T  # transpose pour en faire une colonne\n",
    "similarity_true_doc_vect = sp.csr_matrix(similarity_true).T\n",
    "# Puis on concatène horizontalement\n",
    "fake_features = sp.hstack([fake_features,similarity_fake_doc_vect])\n",
    "true_features = sp.hstack([true_features, similarity_true_doc_vect])\n",
    "\n",
    "from scipy.sparse import save_npz\n",
    "\n",
    "save_npz('fake_features.npz', fake_features)\n",
    "save_npz('true_features.npz', true_features)\n",
    "print(fake_features.shape)  # (nombre d'articles, taille_title + taille_text + 1)\n",
    "print(true_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_supp_features = sp.hstack([title_fake_docvect,text_fake_docvect,similarity_fake_doc_vect])\n",
    "true_supp_features = sp.hstack([title_true_docvect,text_true_docvect,similarity_true_doc_vect])\n",
    "save_npz('fake_supp_features.npz', fake_supp_features)\n",
    "save_npz('true_supp_features.npz', true_supp_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook a accompli un travail de préparation de données substantiel, transformant des textes bruts en divers ensembles de caractéristiques numériques. Ces caractéristiques, notamment `fake_features.npz` et `true_features.npz` (basées sur TF-IDF et augmentées par des similarités) ainsi que `fake_supp_features.npz` et `true_supp_features.npz` (basées sur Doc2Vec), sont maintenant prêtes pour la phase de modélisation en vue de la détection de fausses nouvelles."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
