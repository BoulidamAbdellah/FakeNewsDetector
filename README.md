# D√©tection de Fausses Nouvelles : Performances Compar√©es de Mod√®les d'Apprentissage Automatique

## üìñ √Ä Propos du Projet

Ce projet vise √† d√©velopper et √©valuer un syst√®me de d√©tection de fausses nouvelles (fake news) bas√© sur des techniques d'apprentissage automatique et de traitement du langage naturel (NLP). L'objectif principal est de comparer les performances de diff√©rents mod√®les de classification pour distinguer les articles de presse authentiques des articles fallacieux, en se basant sur leur contenu textuel (titre et corps de l'article).

Le pipeline complet du projet comprend :
1.  **Collecte et Pr√©traitement des Donn√©es** : Utilisation de jeux de donn√©es publics contenant des nouvelles √©tiquet√©es "vraies" et "fausses".
2.  **Nettoyage Approfondi du Texte** : Normalisation (minuscules, suppression de la ponctuation), suppression d'une liste exhaustive de mots vides (stopwords) et lemmatisation.
3.  **Ing√©nierie des Caract√©ristiques (Feature Engineering)** :
    *   Extraction de caract√©ristiques **TF-IDF** (Term Frequency-Inverse Document Frequency) en consid√©rant les unigrammes et bigrammes.
    *   Apprentissage de plongements de documents avec **Doc2Vec**.
    *   Calcul de la **similarit√© cosinus** entre les repr√©sentations vectorielles des titres et des corps de texte (pour TF-IDF et Doc2Vec).
    *   Combinaison de ces √©l√©ments pour former des ensembles de caract√©ristiques robustes.
4.  **Entra√Ænement et √âvaluation de Mod√®les** :
    *   R√©gression Logistique
    *   Machine √† Vecteurs de Support (SVM)
    *   R√©seau de Neurones Profonds (avec Keras/TensorFlow)
5.  **Application Web Interactive** : D√©veloppement d'une interface utilisateur avec Streamlit pour permettre la recherche d'articles et leur v√©rification √† la demande √† l'aide des mod√®les entra√Æn√©s.

Ce d√©p√¥t contient le code source des notebooks Jupyter pour le traitement des donn√©es et l'entra√Ænement des mod√®les, ainsi que le code de l'application Streamlit.

## üöÄ Fonctionnalit√©s Cl√©s

*   **Pipeline de Pr√©traitement Textuel Complet** : Du nettoyage de base √† la lemmatisation avanc√©e.
*   **Ing√©nierie de Caract√©ristiques Multiples** : Combine la puissance de TF-IDF et la s√©mantique de Doc2Vec.
*   **Comparaison de Mod√®les de Classification** : √âvaluation rigoureuse de plusieurs algorithmes d'apprentissage automatique.
*   **Application Streamlit Interactive** :
    *   Recherche d'articles de presse via l'API Google Fact Check Tools.
    *   S√©lection du mod√®le de classification (SVM, R√©gression Logistique, Deep Learning).
    *   V√©rification √† la demande de la v√©racit√© des articles.
    *   Affichage clair des pr√©dictions et (le cas √©ch√©ant) des scores de confiance.

## üõ†Ô∏è Technologies et Biblioth√®ques Utilis√©es

*   **Langage** : Python 3
*   **Manipulation de Donn√©es** : Pandas, NumPy
*   **Traitement du Langage Naturel (NLP)** :
    *   NLTK (stopwords, tokenisation)
    *   spaCy (lemmatisation, traitement efficace du texte)
    *   Gensim (Doc2Vec)
    *   Scikit-learn (TfidfVectorizer, gestion des stopwords)
*   **Apprentissage Automatique** :
    *   Scikit-learn (LogisticRegression, SVC, m√©triques d'√©valuation, train_test_split)
    *   TensorFlow / Keras (mod√®les de r√©seaux de neurones profonds)
*   **Application Web** : Streamlit
*   **Utilitaires** : Joblib (sauvegarde/chargement de mod√®les), tqdm (barres de progression), Matplotlib & Seaborn (visualisations)
*   **API** : Google Fact Check Tools API (via la biblioth√®que `requests`)

## üìÇ Structure du D√©p√¥t

```
.
‚îú‚îÄ‚îÄ data/                           # (Optionnel) Peut contenir les jeux de donn√©es bruts (Fake.csv, True.csv) - Attention √† la taille !
‚îú‚îÄ‚îÄ notebooks/                      # Contient les notebooks Jupyter
‚îÇ   ‚îú‚îÄ‚îÄ 1_data_cleaning_preprocessing.ipynb # Nettoyage, pr√©traitement, ing√©nierie des caract√©ristiques
‚îÇ   ‚îî‚îÄ‚îÄ 2_model_training_evaluation.ipynb # Entra√Ænement et √©valuation des mod√®les
‚îú‚îÄ‚îÄ models/                         # Mod√®les entra√Æn√©s sauvegard√©s (.joblib, .h5)
‚îÇ   ‚îú‚îÄ‚îÄ Lregression_model.joblib
‚îÇ   ‚îú‚îÄ‚îÄ svm_model.joblib
‚îÇ   ‚îî‚îÄ‚îÄ deepmodelclassifier.h5
‚îú‚îÄ‚îÄ app.py                          # Code source de l'application Streamlit
‚îú‚îÄ‚îÄ requirements.txt                # D√©pendances Python n√©cessaires
‚îú‚îÄ‚îÄ README.md                       # Ce fichier
‚îî‚îÄ‚îÄ .gitignore                      # Fichiers et dossiers √† ignorer par Git (ex: __pycache__, venv)
```
*(Note : Vous devrez peut-√™tre ajuster cette structure pour correspondre exactement √† votre d√©p√¥t.)*

## ‚öôÔ∏è Installation et Configuration

1.  **Cloner le d√©p√¥t :**
    ```bash
    git clone https://github.com/VOTRE_NOM_UTILISATEUR/NOM_DE_VOTRE_DEPOT.git
    cd NOM_DE_VOTRE_DEPOT
    ```

2.  **Cr√©er un environnement virtuel (recommand√©) :**
    ```bash
    python -m venv venv
    # Activer l'environnement :
    # Sur Windows :
    # .\venv\Scripts\activate
    # Sur macOS/Linux :
    # source venv/bin/activate
    ```

3.  **Installer les d√©pendances :**
    ```bash
    pip install -r requirements.txt
    ```

4.  **T√©l√©charger les ressources NLTK et spaCy :**
    Ex√©cutez les commandes suivantes dans un interpr√©teur Python ou au d√©but de vos notebooks :
    ```python
    import nltk
    import spacy

    nltk.download('stopwords')
    spacy.cli.download('en_core_web_sm')
    ```

5.  **Jeux de Donn√©es :**
    *   Placez les fichiers `Fake.csv` et `True.csv` dans un r√©pertoire appropri√© (par exemple, un dossier `data/` √† la racine du projet) si vous ne les incluez pas directement dans le suivi Git en raison de leur taille. Mettez √† jour les chemins dans les notebooks si n√©cessaire.
    *   *Alternativement, fournissez un lien de t√©l√©chargement ou des instructions pour obtenir les jeux de donn√©es.*

6.  **Cl√© API Google Fact Check Tools (pour l'application Streamlit) :**
    *   L'application Streamlit utilise l'API Google Fact Check Tools. Vous devrez obtenir votre propre cl√© API.
    *   La cl√© est actuellement cod√©e en dur dans le script `app.py` (`api_key = "AIzaSyCqaCJWmHIZtuX9hCA-k418rqZjXUB_2z8"`). **Il est fortement recommand√© de ne PAS laisser de cl√©s API en clair dans le code sur un d√©p√¥t public.**
    *   Utilisez plut√¥t des variables d'environnement ou un fichier de configuration (ignor√© par Git) pour g√©rer votre cl√© API. Par exemple, avec Streamlit Secrets pour les applications d√©ploy√©es.

## üöÄ Ex√©cution

### 1. Notebooks Jupyter
Ouvrez et ex√©cutez les notebooks dans l'ordre :
1.  `notebooks/1_data_cleaning_preprocessing.ipynb` : Cette √©tape g√©n√©rera les fichiers de caract√©ristiques (`.npz`) et les mod√®les de vectorisation (`.pkl`) n√©cessaires pour l'√©tape suivante. Assurez-vous que les fichiers de donn√©es d'entr√©e (`Fake.csv`, `True.csv`) sont au bon emplacement.
2.  `notebooks/2_model_training_evaluation.ipynb` : Ce notebook charge les caract√©ristiques pr√©trait√©es, entra√Æne les diff√©rents mod√®les de classification, les √©value et sauvegarde les mod√®les entra√Æn√©s dans le dossier `models/`.

### 2. Application Streamlit
Une fois les mod√®les entra√Æn√©s et sauvegard√©s dans le dossier `models/` (et les fichiers `tf_idf_vectorizer.pkl` et `mon_modele_doc2vec.model` disponibles √† la racine ou dans un chemin accessible), vous pouvez lancer l'application Streamlit :
```bash
streamlit run app.py
```
Ouvrez votre navigateur web √† l'adresse locale fournie (g√©n√©ralement `http://localhost:8501`).

## üìä R√©sultats Attendus (Exemple)

Les mod√®les de R√©gression Logistique et d'Apprentissage Profond ont montr√© des performances exceptionnelles sur les jeux de donn√©es utilis√©s, avec des pr√©cisions (accuracy) sup√©rieures √† 99%. Le mod√®le SVM a √©galement bien perform√©, bien que l√©g√®rement en retrait, potentiellement en raison d'une configuration non exhaustive des hyperparam√®tres.

Consultez le rapport complet du projet (si disponible/li√©) pour une analyse d√©taill√©e des performances et des visualisations.

## ü§ù Contribution

Les contributions, suggestions et rapports de bugs sont les bienvenus ! N'h√©sitez pas √† ouvrir une "issue" ou √† proposer une "pull request".

## üìú Licence

Ce projet est sous licence [NOM_DE_LA_LICENCE - ex: MIT License]. Voir le fichier `LICENSE` (si vous en ajoutez un) pour plus de d√©tails.

## üôè Remerciements (Optionnel)

*   Remerciements √† [Nom de l'encadrant(e)] pour ses conseils et son soutien.
*   Sources des jeux de donn√©es (si diff√©rentes de celles implicites).

---
*Derni√®re mise √† jour : [Date]*
```

---

## Proposition pour `requirements.txt`

```txt
pandas
numpy
scikit-learn
joblib
nltk
spacy
gensim
tqdm
tensorflow # Ou tensorflow-cpu si vous n'avez pas besoin du GPU
streamlit
requests
matplotlib # Si vous g√©n√©rez des graphiques dans les notebooks et voulez fixer la version
seaborn    # Si vous g√©n√©rez des graphiques dans les notebooks et voulez fixer la version

# Pour le t√©l√©chargement du mod√®le spaCy via pip (optionnel, mais peut √™tre utile)
# spacy[en_core_web_sm]
```

**Explications et points importants pour le `requirements.txt` :**

*   **Versions :** Pour une meilleure reproductibilit√©, il est fortement recommand√© de sp√©cifier les versions exactes des biblioth√®ques. Vous pouvez g√©n√©rer cela automatiquement apr√®s avoir configur√© votre environnement et install√© tout ce dont vous avez besoin :
    ```bash
    pip freeze > requirements.txt
    ```
    Cela listera toutes les biblioth√®ques de votre environnement avec leurs versions exactes. Vous pourriez ensuite nettoyer ce fichier pour ne garder que les d√©pendances directes de votre projet si vous le souhaitez, mais inclure toutes les d√©pendances fig√©es est plus s√ªr pour la reproductibilit√©.
*   **TensorFlow :** J'ai mis `tensorflow`. Si votre projet n'utilise pas de GPU ou si vous voulez une installation plus l√©g√®re, vous pouvez sp√©cifier `tensorflow-cpu`.
*   **Matplotlib/Seaborn :** Inclus au cas o√π vous les utiliseriez directement dans les notebooks pour des visualisations qui ne sont pas couvertes par les fonctions de plot du code d'entra√Ænement.
*   **`spacy[en_core_web_sm]` :** C'est une mani√®re de s'assurer que le mod√®le `en_core_web_sm` est t√©l√©charg√© lors de l'installation via pip, mais la m√©thode `spacy.cli.download('en_core_web_sm')` reste souvent plus explicite.

---

**Conseils suppl√©mentaires pour votre d√©p√¥t GitHub :**

1.  **`.gitignore` :** Assurez-vous d'avoir un fichier `.gitignore` pour exclure les fichiers inutiles ou sensibles (comme les environnements virtuels `venv/`, les caches Python `__pycache__/`, les fichiers de configuration locaux avec des cl√©s API, les gros fichiers de donn√©es si vous ne voulez pas les versionner).
2.  **Licence :** Si c'est un projet public, ajoutez un fichier `LICENSE` (par exemple, MIT, Apache 2.0).
3.  **Chemins de fichiers :** V√©rifiez que tous les chemins de fichiers dans votre code (notebooks, `app.py`) sont relatifs √† la racine du projet ou g√©r√©s de mani√®re √† ce que d'autres utilisateurs puissent ex√©cuter le code sans modifications majeures.
4.  **Taille des donn√©es :** Les fichiers CSV peuvent √™tre volumineux. GitHub a des limites de taille pour les fichiers et les d√©p√¥ts. Si vos fichiers de donn√©es sont trop gros, envisagez de les h√©berger ailleurs (par exemple, Google Drive, Kaggle Datasets, AWS S3) et de fournir des instructions de t√©l√©chargement dans le README.
5.  **Cl√©s API :** **R√©p√©tez : Ne committez jamais de cl√©s API directement dans votre code sur un d√©p√¥t public.** Utilisez des variables d'environnement, Streamlit Secrets, ou des fichiers de configuration locaux (ajout√©s au `.gitignore`).

J'esp√®re que cela vous aidera √† cr√©er un excellent d√©p√¥t GitHub pour votre projet !
